# Поиск максимального элемента в векторе целых чисел

- Студент: Борцова Ангелина Сергеевна, группа 3823Б1ПР5
- Технология: MPI
- Вариант: 3

## 1. Введение

Сортировка является одной из фундаментальных операций в компьютерных науках, применяемой в базах данных, обработке сигналов, научных вычислениях и многих других областях. Сортировка Шелла — это эффективный алгоритм сортировки с временной сложностью между O(n log n) и O(n²), который использует предварительную сортировку элементов на определенных расстояниях. Четно-нечетное слияние Бэтчера — это алгоритм сортировки слиянием, разработанный специально для параллельных вычислений.

Целью лабораторной работы является разработка и реализация гибридного алгоритма сортировки, сочетающего сортировку Шелла с четно-нечетным слиянием Бэтчера, с последующей параллелизацией с использованием технологии MPI и анализом производительности.

## 2. Постановка задачи

**Входные данные:** вектор чисел типа `double`

**Выходные данные:** отсортированные вектор чисел типа 'double'.

**Ограничения:**
- Размер входного вектора должен быть больше нуля
- Элементы вектора — вещественные числа типа double
- Алгоритм должен корректно обрабатывать повторяющиеся элементы
- Должна быть обеспечена стабильность сортировки

## 3. Базовый алгоритм (последовательный)

Последовательный алгоритм состоит из двух основных компонентов:
1. **Сортировка Шелла:** 
    - Используется последовательность расстояний (gaps): {701, 301, 132, 57, 23, 10, 4, 1}
    - Для каждого расстояния выполняется сортировка вставками подмассивов
    - Алгоритм начинается с наибольшего расстояния и уменьшает его до 1

2. **Четно-нечетное слияние Бэтчера:**
    - Разделение массива на четные и нечетные элементы
    - Рекурсивная сортировка каждой подпоследовательности
    - Слияние с финальной проверкой соседних элементов

```c++
function ShellSortBatcherMerge(array):
    gaps = [701, 301, 132, 57, 23, 10, 4, 1]
    
    for each gap in gaps:
        for i from gap to n-1:
            temp = array[i]
            j = i
            while j >= gap and array[j-gap] > temp:
                array[j] = array[j-gap]
                j -= gap
            array[j] = temp
        
        if gap <= n/2:
            BatcherMerge(array, 0, n/2-1, n-1)
    
    BatcherMerge(array, 0, n/2-1, n-1)
    return array

function BatcherMerge(array, left, middle, right):
    size = right - left + 1
    if size <= 1: return
    if size == 2:
        if array[left] > array[right]: swap(array[left], array[right])
        return
    
    block = 1
    while block < size:
        for start from left to right step 2*block:
            for i from 0 to block-1:
                if start+i+block <= right and array[start+i] > array[start+i+block]:
                    swap(array[start+i], array[start+i+block])
        
        for start from left+block to right step 2*block:
            for i from 0 to block-1:
                if start+i+block <= right and array[start+i] > array[start+i+block]:
                    swap(array[start+i], array[start+i+block])
        
        block *= 2
    
    for i from left to right-1:
        if array[i] > array[i+1]: swap(array[i], array[i+1])
```

**Сложность:**
- Временная сложность: O(n log²n) в среднем случае
- Пространственная сложность: O(n) для вспомогательных массивов

## 4. Схема распараллеливания

### 4.1 Общая схема

1. **Создание оптимизированного коммуникатора**
   - Определение максимальной степени двойки, не превышающей общее количество процессов
   - Создание отдельного коммуникатора для процессов, участвующих в параллельной сортировке

2. **Распределение данных (Scatterv)**
   - Корневой процесс владеет полным массивом данных
   - Массив разделяется на части с учетом остатка при делении
   - Каждый процесс получает свою часть через `MPI_Scatterv`

3. **Локальная сортировка**
   - Каждый процесс независимо сортирует свою часть
   - Используется гибридный алгоритм: сортировка Шелла + локальное слияние Бэтчера

4. **Параллельное четно-нечетное слияние**
    - Процессы в коммуникаторе степени двойки выполняют параллельную сортировку Бэтчера
    - Используется итеративный алгоритм с обменом данными между партнерами

5. **Сбор результатов (Gatherv)**
    - Все отсортированные части собираются на корневом процессе
    - Выполняется финальное последовательное слияние всех частей

### 4.2 Распределение нагрузки

При количестве процессов `p` и размере данных `n`:
- Базовый размер блока: `base_size  = n / p`
- Остаток: `remainder = n % p`
- Первые `remainder` процессов получают по `base_size  + 1` элементу
- Остальные процессы получают по `base_size ` элементов
Это обеспечивает равномерное распределение даже при `n` не кратном `p`.

### 4.3 Коммуникационная топология и роли процессов

**Роли процессов:**
- **Процесс 0:** Владеет исходными данными, распределяет части, собирает результаты, выполняет финальное слияние
- **Остальные процессы (rank 1..p-1):** Владеет исходными данными, распределяет части, собирает результаты, выполняет финальное слияние

**Схема взаимодействия:**

```
Шаг 1: Инициализация
    Все процессы: получение конфигурации
    
Шаг 2: Создание коммуникатора
    Все процессы: определение power = max(2^k ≤ p)
    Процессы 0..power-1: создание нового коммуникатора
    
Шаг 3: Распределение данных
    Процесс 0: расчет размеров и смещений
    Все процессы: получение локальной части через MPI_Scatterv
    
Шаг 4: Локальная сортировка
    Все процессы: сортировка Шелла + локальное слияние Бэтчера
    
Шаг 5: Параллельное слияние (только для процессов в коммуникаторе)
    Для step = 1, 2, 4, ... < power:
        Для offset = step, step/2, step/4, ... > 0:
            Определение partner = rank ^ offset
            Если partner < power:
                Обмен данными с партнером
                Слияние локальных данных с полученными
    
Шаг 6: Сбор результатов
    Все процессы: отправка данных на процесс 0 через MPI_Gatherv
    Процесс 0: последовательное слияние всех частей
```

### 4.4 Синхронизация

- `MPI_Barrier`: синхронизация после каждого этапа параллельного слияния
- `MPI_Sendrecv`: обмен данными между партнерами в алгоритме Бэтчера
- `MPI_Allreduce`: определение максимального размера локального массива

## 5. Детали реализации

### 5.1 Структура кода

Проект организован следующим образом:

**Файлы:**
- `common/include/common.hpp` — общие определения типов данных
- `seq/include/ops_seq.hpp` и `seq/src/ops_seq.cpp` — последовательная версия
- `mpi/include/ops_mpi.hpp` и `mpi/src/ops_mpi.cpp` — параллельная версия 
- `tests/functional/main.cpp` — функциональные тесты

**Ключевые классы:**
- `BortsovaAShellBatcherkSEQ` — последовательная версия задачи
- `BortsovaAShellBatcherkMPI` — параллельная версия задачи 

Оба класса наследуются от базового класса `ppc::task::Task` и реализуют интерфейс:
- `ValidationImpl()` — проверка корректности данных
- `PreProcessingImpl()` — подготовка к выполнению
- `RunImpl()` — основной алгоритм
- `PostProcessingImpl()` — завершение

### 5.2 Структуры данных

```cpp
using InType = std::vector<double>;  // входной вектор (back sorted)
using OutType = std::vector<double>; // отсортированный вектор
using TestType = int;                // значение для инииализации тестов, длина входного вектора
```

### 5.3 Важные особенности реализации

**Оптимизация для степеней двойки:**
    - Алгоритм Бэтчера наиболее эффективен при числе процессов, равном степени двойки
    - Создается отдельный коммуникатор с максимально возможной степенью двойки
    - Процессы вне этого коммуникатора выполняют только локальную сортировку

**Обработка неравных размеров:**
    - Использование `MPI_Scatterv` и `MPI_Gatherv` для поддержки неравномерного распределения
    - Дополнение локальных массивов до одинакового размера для параллельного слияния
    - Восстановление исходных размеров после сортировки

**Эффективное слияние:**
    - Разделение логики слияния на две функции: `MergeSmall` и `MergeLarge`
    - Минимизация выделения памяти за счет предварительного резервирования
    - Использование `std::ranges::copy` для эффективного копирования данных

**Управление памятью:**
    - Локальные данные хранятся в `std::vector` с автоматическим управлением памятью
    - Временные буферы создаются с минимально необходимым размером
    - Освобождение MPI-коммуникаторов после использования

## 6. Экспериментальное окружение

### 6.1 Аппаратное и программное обеспечение

**Аппаратная конфигурация:**
- Процессор: Intel(R) Core(TM) Ultra 7 155H (3.80 GHz)
- Количество ядер/потоков: 16 ядер/22 потока
- Оперативная память: 32 ГБ
- Операционная система: Windows 11 pro

**Программное окружение:**
- Компилятор: MSVC 
- MPI версия: 10.0
- Тип сборки: Release
- CMake версия: 4.2.0-rc1

**Параметры окружения:**
- Количество процессов MPI: 1, 2, 3, 4
- Размер тестового вектора: 250,000,000 чисел
- Тип данных: `int` 
- Объем данных: ~953 МБ 

### 6.2 Генерация тестовых данных

1. **Функциональные тесты:**
    - Тестирование на отсортированных по убыванию векторах
    - Тестирование на веткорах различных размеров

2. **Perfomance-тесты:**
    - Размер данных - `1.5e6` чисел. Это наиболее оптимальное количество: seq реализация выполняется не слишком медленно, mpi - не слишком быстро


## 7. Результаты и обсуждение

### 7.1 Корректность

1. **Сравнение с эталонной функцией:** результат обеих версий программы сравнивается с результатом функции `std::ranges::is_sorted`, которая гарантирует правильность проверки отсортированности массива. 
MAX`

2. **Различные размеры данных:**
   - от 100 до 550 с шагом 50 (входные длины не всегда кратны числу процессов)
   - 1.5e6 - для проверки корректности работы на больших данных

4. **Различное количество процессов:**
   - Проверка корректности при 1, 2, 3, 4 и более процессах
   - Случаи, когда размер данных не делится нацело на количество процессов


### 7.2 Производительность

#### 7.2.1 Замеры времени выполнения в task_run

**Таблица 1. Время выполнения основного алгоритма (task_run)**

| Размер вектора | Режим | Число процессов | Время, с | Ускорение | Эффективность |
|----------------|-------|-----------------|----------|-----------|---------------|
| 250,000,000    | seq   | 1               | 0.2389   | 1.00      | N/A           |
| 250,000,000    | mpi   | 1               | 1.1445   | 0.21      | 20.9%         |
| 250,000,000    | mpi   | 2               | 0.8485   | 0.28      | 14.1%         |
| 250,000,000    | mpi   | 3               | 0.6933   | 0.34      | 11.5%         |
| 250,000,000    | mpi   | 4               | 0.7454   | 0.32      | 8.0%          |

Формулы для расчета:
- **Ускорение (Speedup):** `S = T_seq / T_parallel`
- **Эффективность (Efficiency):** `E = S / p × 100%`, где `p` — количество процессов

#### 7.2.2 Замеры времени выполнения в task_pipeline

**Таблица 2. Время выполнения полного пайплайна (task_pipeline)**

| Размер вектора | Режим | Число процессов | Время, с | Ускорение | Эффективность |
|----------------|-------|-----------------|----------|-----------|---------------|
| 250,000,000    | seq   | 1               | 0.2326   | 1.00      | N/A           |
| 250,000,000    | mpi   | 1               | 1.1489   | 0.20      | 20.2%         |
| 250,000,000    | mpi   | 2               | 0.8377   | 0.28      | 13.9%         |
| 250,000,000    | mpi   | 3               | 0.7191   | 0.32      | 10.8%         |
| 250,000,000    | mpi   | 4               | 0.7336   | 0.32      | 7.9%          |

#### 7.2.3 Анализ результатов

**Наблюдаемое поведение:**

1. **Отсутствие ускорения:**
   - MPI-версия на 1 процессе работает в 4.8 раза медленнее последовательной версии (1.145с vs 0.239с)
   - Даже на 4 процессах MPI-версия медленнее SEQ в 3.1 раза (0.745с vs 0.239с)
   - Ускорение отсутствует из-за превышения накладных расходов над выигрышем от параллелизма

2. **Причины низкой производительности:**
   
   **a) Высокие коммуникационные издержки:**
   - `MPI_Scatterv`: передача ~953 МБ данных от процесса 0 к остальным процессам
   - `MPI_Reduce`: сбор результатов (минимальные затраты - только одно число)
   - Время коммуникации превышает время вычислений локального максимума
   
   **b) Простота вычислений:**
   - Поиск максимума — операция O(n) с минимальными вычислениями на элемент
   - Один проход по массиву с простым сравнением `if (x > max)`
   - Вычислительная интенсивность слишком мала для эффективного параллелизма
   
   **c) Memory bandwidth bottleneck:**
   - Задача ограничена скоростью доступа к памяти (memory-bound)
   - CPU ожидает данные из RAM больше времени, чем выполняет сравнения
   - Параллелизм не помогает при узком месте в пропускной способности памяти

3. **Положительная динамика при увеличении процессов:**
   - При переходе от 1 к 3 процессам наблюдается улучшение (task_run: 1.145с → 0.693с)
   - Лучший результат на 3 процессах для task_run: 0.693 секунды
   - При 4 процессах время немного увеличилось до 0.745с (возможно, из-за оверхеда синхронизации)

4. **Разница между task_run и task_pipeline:**
   - Результаты практически идентичны (различие < 10%)
   - Pipeline включает pre/post processing, но они минимальны для данной задачи
   - Основное время тратится в RunImpl() — непосредственно на поиск максимума

**Выводы по результатам:**

- Корректность: все тесты проходят успешно
- Масштабируемость наблюдается при увеличении числа процессов 

## 8. Выводы

В ходе выполнения лабораторной работы была реализована параллельная версия алгоритма поиска максимального элемента в векторе с использованием технологии MPI и проведен детальный анализ её производительности.

**Достигнутые результаты:**
1. Реализована корректная параллельная версия алгоритма с использованием коллективных операций MPI
2. Обеспечена равномерная балансировка нагрузки между процессами с использованием `MPI_Scatterv`
3. Проведено тестирование на 42 различных наборах данных — все тесты пройдены успешно
4. Выполнены замеры производительности на векторе из 250 миллионов элементов
5. Проведен анализ производительности и выявлены узкие места реализации

**Что работает:**
- **Корректность:** реализация полностью корректна для всех тестовых случаев
- **Архитектура кода:** чистая структура с разделением на seq/mpi версии
- **Балансировка:** равномерное распределение нагрузки даже при n не кратном p
- **Коллективные операции:** правильное использование `MPI_Scatterv` и `MPI_Reduce`

**Ограничения и проблемы:**
- **Отсутствие ускорения:** MPI-версия медленнее последовательной в 3-5 раз
- **Коммуникационные издержки:** передача ~953 МБ данных занимает больше времени, чем вычисления
- **Характер задачи:** поиск максимума — слишком простая операция для параллелизма через MPI
- **Memory-bound:** задача ограничена пропускной способностью памяти, а не вычислительной мощностью

**Практические выводы:**
1. **Не каждая задача выигрывает от параллелизма** — важно оценивать соотношение вычислений к коммуникациям
2. Для задачи поиска максимума лучше подходят:
   - Последовательная реализация на одном процессе
   - SIMD-инструкции (AVX/SSE) для векторизации
   - OpenMP для многопоточности с разделяемой памятью (без накладных расходов на передачу данных)
3. MPI эффективен для задач с высокой вычислительной интенсивностью и минимальными коммуникациями

**Образовательная ценность:**
Несмотря на отсутствие ускорения, работа продемонстрировала:
- Правильное использование коллективных операций MPI
- Понимание накладных расходов параллелизма
- Умение анализировать и объяснять результаты производительности
- Критическое мышление при выборе технологии распараллеливания

## 9. Источники

1. MPI: A Message-Passing Interface Standard, Version 4.1. Message Passing Interface Forum, 2023.  
   URL: https://www.mpi-forum.org/docs/

2. Documentation for MS-MPI (Microsoft MPI).  
   URL: https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi

3. Сысоев А. В. Лекции по параллельному программированию. — Н. Новгород: ННГУ, 2025.

4. Intel MPI Library Developer Reference.  
   URL: https://www.intel.com/content/www/us/en/docs/mpi-library/

5. Open MPI Documentation.  
   URL: https://www.open-mpi.org/doc/

